{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "downtown-concept",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing libraries...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import os.path as osp\n",
    "partial_path = os.getcwd().rsplit(\"/\", 1)[0]\n",
    "sys.path.insert(0, f\"{partial_path}/src/\")\n",
    "sys.path.insert(0, f\"{partial_path}/src/deep-person-reid/\")\n",
    "\n",
    "import torchreid\n",
    "\n",
    "import config as cf\n",
    "from train import train\n",
    "from datasets import train_data, test_data\n",
    "\n",
    "# os.environ[\"LRU_CACHE_CAPACITY\"] = \"3\"\n",
    "# import torch\n",
    "# torch.cuda.set_per_p-rocess_memory_fraction(0.35, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "breeding-milwaukee",
   "metadata": {},
   "source": [
    "## PREPARE DATASETS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "civilian-amino",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample_dataset_encoded_clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f012699cc4242f7b0fcb8f862de3828",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 50\n",
      "Number of query images: 25\n",
      "Number of gallery images: 25\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    25 |       50 |         2\n",
      "  query    |    25 |       25 |         1\n",
      "  gallery  |    25 |       25 |         1\n",
      "  ----------------------------------------\n",
      "subsample_synthetic_dataset_encoded_clean\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1551386d33e4decaf78cd48f2edfb8a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 35\n",
      "Number of query images: 5\n",
      "Number of gallery images: 30\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |     5 |       35 |         7\n",
      "  query    |     5 |        5 |         1\n",
      "  gallery  |     5 |       30 |         6\n",
      "  ----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for path in cf.TRAIN_DATASETS:\n",
    "    print(path)\n",
    "    class train_dataset(train_data):\n",
    "        def __init__(self, root=cf.DATA_ROOT, dataset_dir=path, print_summary=True, **kwargs):\n",
    "            super().__init__(root, dataset_dir, print_summary, **kwargs)\n",
    "    train_dataset(root=cf.DATA_ROOT, dataset_dir=path)\n",
    "    torchreid.data.register_image_dataset(path, train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "smooth-maximum",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subsample_dataset_encoded_test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d49ed6ec5db94166aa1f4a651c32e98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 7\n",
      "Number of query images: 13\n",
      "Number of gallery images: 63\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded test_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |     1 |        7 |         7\n",
      "  query    |    13 |       13 |         1\n",
      "  gallery  |    13 |       63 |         6\n",
      "  ----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for path in cf.TEST_DATASETS:\n",
    "    print(path)\n",
    "    class test_dataset(test_data):\n",
    "        def __init__(self, root=cf.DATA_ROOT, dataset_dir=path, print_summary=True, **kwargs):\n",
    "            super().__init__(root, dataset_dir, print_summary, **kwargs)\n",
    "    test_dataset(root=cf.DATA_ROOT, dataset_dir=path)\n",
    "    torchreid.data.register_image_dataset(path, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "great-angola",
   "metadata": {},
   "source": [
    "## MODEL TRAIN: GRID SEARCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "hybrid-substance",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Building train transforms ...\n",
      "+ resize to 256x256\n",
      "+ random crop (enlarge to 288x288 and crop 256x256)\n",
      "+ random patch\n",
      "+ color jitter\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "Building test transforms ...\n",
      "+ resize to 256x256\n",
      "+ to torch tensor of range [0, 1]\n",
      "+ normalization (mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
      "=> Loading train (source) dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5440b2fcd3224b17a6814372e2e12e23",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 50\n",
      "Number of query images: 25\n",
      "Number of gallery images: 25\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    25 |       50 |         2\n",
      "  query    |    25 |       25 |         1\n",
      "  gallery  |    25 |       25 |         1\n",
      "  ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bebf74cae91d4ad58498150680013ac2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 35\n",
      "Number of query images: 5\n",
      "Number of gallery images: 30\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |     5 |       35 |         7\n",
      "  query    |     5 |        5 |         1\n",
      "  gallery  |     5 |       30 |         6\n",
      "  ----------------------------------------\n",
      "=> Loading test (target) dataset\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8bb051cd12fc4532903fad428c5806a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 50\n",
      "Number of query images: 25\n",
      "Number of gallery images: 25\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |    25 |       50 |         2\n",
      "  query    |    25 |       25 |         1\n",
      "  gallery  |    25 |       25 |         1\n",
      "  ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f9d633ada5b42b2b6875226f0d46c28",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 50\n",
      "Number of query images: 25\n",
      "Number of gallery images: 25\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c39da348ea984bf5939e03ce2d31a9b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 35\n",
      "Number of query images: 5\n",
      "Number of gallery images: 30\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded train_dataset\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |     5 |       35 |         7\n",
      "  query    |     5 |        5 |         1\n",
      "  gallery  |     5 |       30 |         6\n",
      "  ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97bacb2affe543b891054573df244150",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/70 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 35\n",
      "Number of query images: 5\n",
      "Number of gallery images: 30\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc5fa00c49b7489d82336dea88ae052f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 7\n",
      "Number of query images: 13\n",
      "Number of gallery images: 63\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "=> Loaded test_data\n",
      "  ----------------------------------------\n",
      "  subset   | # ids | # images | # cameras\n",
      "  ----------------------------------------\n",
      "  train    |     1 |        7 |         7\n",
      "  query    |    13 |       13 |         1\n",
      "  gallery  |    13 |       63 |         6\n",
      "  ----------------------------------------\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "389f2f2b17184fc3ae97d1f8db9b55d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/76 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "Number of train images: 7\n",
      "Number of query images: 13\n",
      "Number of gallery images: 63\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "\n",
      "\n",
      "  **************** Summary ****************\n",
      "  source            : ['subsample_dataset_encoded_clean', 'subsample_synthetic_dataset_encoded_clean']\n",
      "  # source datasets : 2\n",
      "  # source ids      : 30\n",
      "  # source images   : 85\n",
      "  # source cameras  : 9\n",
      "  target            : ['subsample_dataset_encoded_clean', 'subsample_synthetic_dataset_encoded_clean', 'subsample_dataset_encoded_test']\n",
      "  *****************************************\n",
      "\n",
      "\n",
      "Successfully loaded imagenet pretrained weights from \"/home/e3eferri/.cache/torch/checkpoints/osnet_ain_x1_0_imagenet.pth\"\n",
      "** The following layers are discarded due to unmatched keys or layer size: ['classifier.weight', 'classifier.bias']\n",
      "=> Start training\n",
      "##### Evaluating subsample_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Speed: 0.0161 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 25\n",
      "** Results **\n",
      "mAP: 94.0%\n",
      "CMC curve\n",
      "Rank-1  : 88.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_synthetic_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 5-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 30-by-512 matrix\n",
      "Speed: 0.0171 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 30\n",
      "** Results **\n",
      "mAP: 90.9%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_dataset_encoded_test (target) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 13-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 63-by-512 matrix\n",
      "Speed: 0.0172 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "** Results **\n",
      "mAP: 87.3%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "Checkpoint saved to \"../models/model_train/model_train/model/model.pth.tar-1\"\n",
      "##### Evaluating subsample_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Speed: 0.0161 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 25\n",
      "** Results **\n",
      "mAP: 96.0%\n",
      "CMC curve\n",
      "Rank-1  : 92.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_synthetic_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 5-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 30-by-512 matrix\n",
      "Speed: 0.0175 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 30\n",
      "** Results **\n",
      "mAP: 92.2%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_dataset_encoded_test (target) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 13-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 63-by-512 matrix\n",
      "Speed: 0.0167 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "** Results **\n",
      "mAP: 91.3%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "Checkpoint saved to \"../models/model_train/model_train/model/model.pth.tar-2\"\n",
      "=> Final test\n",
      "##### Evaluating subsample_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 25-by-512 matrix\n",
      "Speed: 0.0164 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 25\n",
      "** Results **\n",
      "mAP: 98.0%\n",
      "CMC curve\n",
      "Rank-1  : 96.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_synthetic_dataset_encoded_clean (source) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 5-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 30-by-512 matrix\n",
      "Speed: 0.0177 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "Note: number of gallery samples is quite small, got 30\n",
      "** Results **\n",
      "mAP: 93.2%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "##### Evaluating subsample_dataset_encoded_test (target) #####\n",
      "Extracting features from query set ...\n",
      "Done, obtained 13-by-512 matrix\n",
      "Extracting features from gallery set ...\n",
      "Done, obtained 63-by-512 matrix\n",
      "Speed: 0.0166 sec/batch\n",
      "Computing distance matrix with metric=cosine ...\n",
      "Computing CMC and mAP ...\n",
      "** Results **\n",
      "mAP: 93.8%\n",
      "CMC curve\n",
      "Rank-1  : 100.0%\n",
      "Rank-5  : 100.0%\n",
      "Rank-10 : 100.0%\n",
      "Rank-20 : 100.0%\n",
      "Checkpoint saved to \"../models/model_train/model_train/model/model.pth.tar-3\"\n",
      "Elapsed 0:00:09\n"
     ]
    }
   ],
   "source": [
    "train(cf.parameters_train)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "8b8f3989125208fea901a3daa574ecf0772df957395fd275df331599c4a0898c"
  },
  "kernelspec": {
   "display_name": ".conda-torchreid",
   "language": "python",
   "name": "conda-env-.conda-torchreid-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
